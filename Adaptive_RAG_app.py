import streamlit as st
from Adaptive_RAG import graph, format_output
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Adaptive RAG Demo",
    page_icon="ğŸ¤–",
    layout="wide"
)


# ä¾§è¾¹æ é…ç½®
st.sidebar.title("âš™ï¸ Configuration")

# æ¨¡å‹é…ç½®
st.sidebar.subheader("Model Settings")
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.0, 0.1)
max_retries = st.sidebar.slider("Max Retries", 1, 5, 3)

# æ£€ç´¢é…ç½®
st.sidebar.subheader("Retrieval Settings")
k_docs = st.sidebar.slider("Number of Documents to Retrieve", 1, 10, 3)

# æ•°æ®æºæ˜¾ç¤º
st.sidebar.subheader("Available Data Sources")
st.sidebar.markdown("""
- ğŸ“š Vector Store (Local Documents)
- ğŸŒ Web Search (Tavily API)
""")

# ç³»ç»ŸçŠ¶æ€æŒ‡ç¤ºå™¨
system_status = st.sidebar.empty()
system_status.success("System Ready")

# ä¸»ç•Œé¢
col1, col2 = st.columns([2, 1])

with col1:
    
    # æ·»åŠ æ ‡é¢˜å’Œæè¿°
    st.title("ğŸ¤– Adaptive RAG System")
    st.markdown("""
    This is an advanced RAG system that can:
    - Dynamically choose between vector store and web search
    - Check for hallucinations
    - Validate answer relevance
    - Retry if necessary
    """)

    # è¾“å…¥åŒºåŸŸ
    st.subheader("ğŸ” Ask Your Question")
    user_question = st.text_area(
        "Enter your question here:",
        height=100,
        placeholder="e.g., What are the latest developments in LLM agents?"
    )
    
    # æäº¤æŒ‰é’®
    submit = st.button("ğŸš€ Submit", type="primary")

    if submit and user_question:
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # å‡†å¤‡è¾“å…¥
        inputs = {
            "question": user_question,
            "max_retries": max_retries
        }
        
        # åˆ›å»ºç»“æœå®¹å™¨
        result_container = st.empty()
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            steps = 0
            for event in graph.stream(inputs, stream_mode="values"):
                steps += 1
                progress = min(steps / 5, 1.0)  # å‡è®¾æœ€å¤š5ä¸ªæ­¥éª¤
                progress_bar.progress(progress)
                
                # æ›´æ–°çŠ¶æ€
                if "documents" in event:
                    status_text.info("ğŸ“‘ Retrieved relevant documents...")
                elif "web_search" in event:
                    status_text.info("ğŸŒ Performing web search...")
                elif "generation" in event:
                    status_text.info("âœï¸ Generating response...")
                
                # æ˜¾ç¤ºç»“æœ
                output = format_output(event)
                if output:
                    result_container.markdown(f"### ğŸ’¡ Answer:\n{output}")
                
                time.sleep(0.5)  # æ·»åŠ å°å»¶è¿Ÿä»¥å±•ç¤ºè¿›åº¦
            
            # å®Œæˆ
            progress_bar.progress(1.0)
            status_text.success("âœ… Processing complete!")
            
        except Exception as e:
            st.error(f"An error occurred: {str(e)}")
            system_status.error("System Error")

with col2:
    # ç³»ç»Ÿä¿¡æ¯å±•ç¤º
    st.subheader("ğŸ“Š System Information")
    
    # æ·»åŠ æŒ‡æ ‡
    col_a, col_b = st.columns(2)
    with col_a:
        st.metric(label="Model", value="LLama 3.1")
    with col_b:
        st.metric(label="Embeddings", value="Nomic")
    
    # æ·»åŠ å¤„ç†æµç¨‹å±•ç¤º
    st.subheader("ğŸ”„ Processing Flow")
    st.markdown("""
    1. **Question Analysis**
       - Route to appropriate data source
    2. **Document Retrieval**
       - Vector store or web search
    3. **Relevance Check**
       - Grade document relevance
    4. **Answer Generation**
       - Generate concise response
    5. **Quality Control**
       - Check for hallucinations
       - Verify answer relevance
    """)

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("*Built with Streamlit & LangGraph ğŸš€*")