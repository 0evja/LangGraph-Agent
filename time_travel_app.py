# app_frontend.py
import streamlit as st
from time_travel import graph, State, MemorySaver  # å¯¼å…¥åŸå§‹ä¸šåŠ¡é€»è¾‘
from langchain_core.messages import AIMessage, HumanMessage
import uuid

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹",
    page_icon="ğŸ”",
    layout="centered",
    initial_sidebar_state="expanded"
)

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ é…ç½®è®¾ç½®")
    
    # ä¼šè¯ç®¡ç†
    st.subheader("ä¼šè¯ç®¡ç†")
    if st.button("æ–°å»ºä¼šè¯"):
        st.session_state.clear()
    
    # æ¨¡å‹è®¾ç½®
    st.subheader("æ¨¡å‹è®¾ç½®")
    ollama_model = st.selectbox(
        "é€‰æ‹©AIæ¨¡å‹",
        ["llama3.1:8b", "llama2", "mistral"],
        index=0,
        help="é€‰æ‹©ä½¿ç”¨çš„Ollamaæ¨¡å‹"
    )
    
    # é«˜çº§è®¾ç½®
    st.subheader("é«˜çº§è®¾ç½®")
    max_results = st.slider("æœ€å¤§æœç´¢ç»“æœ", 1, 5, 2)
    temperature = st.slider("æ¨¡å‹æ¸©åº¦", 0.0, 1.0, 0.7)
    stream_mode = st.checkbox("å¯ç”¨å®æ—¶æµå¼ä¼ è¾“", True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

# ä¸»ç•Œé¢å¸ƒå±€
st.title("ğŸ” æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹")
st.caption("ä½¿ç”¨LangGraphå’ŒOllamaæ„å»ºçš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œæ”¯æŒå®æ—¶ç½‘ç»œæœç´¢")

# æ˜¾ç¤ºèŠå¤©è®°å½•
for msg in st.session_state.messages:
    role = "AI" if isinstance(msg, AIMessage) else "ç”¨æˆ·"
    with st.chat_message(role):
        st.markdown(msg.content)
        if hasattr(msg, "additional_info"):
            st.json(msg.additional_info)

# ç”¨æˆ·è¾“å…¥å¤„ç†
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶é—®é¢˜"):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    user_msg = HumanMessage(content=prompt)
    st.session_state.messages.append(user_msg)
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # å‡†å¤‡AIå“åº”åŒºåŸŸ
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        # æ„å»ºé…ç½®
        config = {
            "configurable": {
                "thread_id": st.session_state.thread_id,
                "model_settings": {
                    "temperature": temperature,
                    "max_results": max_results
                }
            }
        }
        
        # è¿è¡Œå¤„ç†æµç¨‹
        try:
            events = graph.stream(
                {"messages": [user_msg]},
                config=config,
                stream_mode="values" if stream_mode else "updates"
            )
            
            # å¤„ç†å®æ—¶äº‹ä»¶
            for event in events:
                if "messages" in event:
                    latest_msg = event["messages"][-1]
                    full_response += latest_msg.content + " "
                    
                    # æ›´æ–°å®æ—¶æ˜¾ç¤º
                    response_placeholder.markdown(full_response + "â–Œ")
                    
                    # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
                    if hasattr(latest_msg, "tool_calls"):
                        with st.expander("æŸ¥çœ‹ç ”ç©¶è¿‡ç¨‹"):
                            for tool_call in latest_msg.tool_calls:
                                st.write(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call['name']}")
                                st.json(tool_call["args"])
            
            # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´å“åº”
            response_placeholder.markdown(full_response)
            
            # ä¿å­˜AIæ¶ˆæ¯
            ai_msg = AIMessage(content=full_response)
            st.session_state.messages.append(ai_msg)
            
        except Exception as e:
            st.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

# ä¾§è¾¹æ åº•éƒ¨ä¿¡æ¯
with st.sidebar:
    st.divider()
    st.markdown("""
    **åŠŸèƒ½ç‰¹æ€§ï¼š**
    - å¤šæ¨¡å‹æ”¯æŒ
    - å®æ—¶ç½‘ç»œç ”ç©¶
    - å¯¹è¯å†å²ç®¡ç†
    - å¯è°ƒèŠ‚å‚æ•°é…ç½®
    """)